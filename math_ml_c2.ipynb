{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TseZF58nzQV",
        "outputId": "96755065-f865-4870-dbd8-7890124cd89f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n"
          ]
        }
      ],
      "source": [
        "def my_function(x):\n",
        "    return x**2 + 2*x + 1\n",
        "\n",
        "result = my_function(3)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sympy as sp\n",
        "\n",
        "x = sp.symbols('x')\n",
        "expression = x**2 + 2*x + 1\n",
        "print(\"Expression:\", expression)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZeADxOPn88h",
        "outputId": "8d97692b-50a0-49f8-9a76-6e151d2fde34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expression: x**2 + 2*x + 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Differentiate with respect to x\n",
        "derivative = sp.diff(expression, x)\n",
        "print(\"Derivative:\", derivative)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnQAguj0n_en",
        "outputId": "362a889a-fcb3-4194-b862-e4a75fddbbd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Derivative: 2*x + 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def f(x):\n",
        "    return x**2 + 2*x + 1\n",
        "\n",
        "# Define a small step size\n",
        "h = 1e-5\n",
        "x0 = 3.0\n",
        "numerical_derivative = (f(x0 + h) - f(x0)) / h\n",
        "print(\"Numerical Derivative:\", numerical_derivative)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPSF-0nLoDio",
        "outputId": "167694a8-925b-4bca-fb38-08550e77da4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numerical Derivative: 8.000009999875601\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "\n",
        "def f(x):\n",
        "    return x**2 + 2*x + 1\n",
        "\n",
        "# Compute derivative using JAX's grad\n",
        "dfdx = grad(f)\n",
        "print(\"Automatic Differentiation with JAX:\", dfdx(3.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwaI-OLjoGdv",
        "outputId": "d871ae6e-2ed1-4415-ac40-468790ac76ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatic Differentiation with JAX: 8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function: f(x) = x^2\n",
        "def f(x):\n",
        "    return x**2\n",
        "\n",
        "# Derivative: f'(x) = 2x\n",
        "def df(x):\n",
        "    return 2*x\n",
        "\n",
        "# Gradient Descent parameters\n",
        "learning_rate = 0.1\n",
        "x = 10  # Starting point\n",
        "tolerance = 1e-6\n",
        "max_iterations = 1000\n",
        "\n",
        "# Gradient Descent loop\n",
        "for i in range(max_iterations):\n",
        "    gradient = df(x)\n",
        "    new_x = x - learning_rate * gradient  # Update rule\n",
        "\n",
        "    if abs(new_x - x) < tolerance:  # Convergence check\n",
        "        break\n",
        "    x = new_x\n",
        "\n",
        "print(\"Global minimum found at x =\", x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7Kxe4ioow83",
        "outputId": "d302cb98-ef0f-4d64-ceea-44c634353509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global minimum found at x = 4.017345110647478e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function: f(x) = cos(x)\n",
        "def f(x):\n",
        "    return np.cos(x)\n",
        "\n",
        "# Derivative: f'(x) = -sin(x)\n",
        "def df(x):\n",
        "    return -np.sin(x)\n",
        "\n",
        "# Gradient Descent parameters\n",
        "learning_rate = 0.1\n",
        "x = 2  # Starting point near a local minimum\n",
        "tolerance = 1e-6\n",
        "max_iterations = 1000\n",
        "\n",
        "# Gradient Descent loop\n",
        "for i in range(max_iterations):\n",
        "    gradient = df(x)\n",
        "    new_x = x - learning_rate * gradient  # Update rule\n",
        "\n",
        "    if abs(new_x - x) < tolerance:  # Convergence check\n",
        "        break\n",
        "    x = new_x\n",
        "\n",
        "print(\"Local minimum found at x =\", x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYJE262nozPf",
        "outputId": "8e8114e3-8c84-4ad7-aca0-7151716de76d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local minimum found at x = 3.141582832390514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function: f(x, y) = x^2 + y^2\n",
        "def f(x, y):\n",
        "    return x**2 + y**2\n",
        "\n",
        "# Gradient of f(x, y): df/dx = 2x, df/dy = 2y\n",
        "def gradient(x, y):\n",
        "    return np.array([2*x, 2*y])\n",
        "\n",
        "# Gradient Descent parameters\n",
        "learning_rate = 0.1\n",
        "point = np.array([10, 5])  # Initial starting point (x0, y0)\n",
        "tolerance = 1e-6\n",
        "max_iterations = 1000\n",
        "\n",
        "# Gradient Descent loop\n",
        "for i in range(max_iterations):\n",
        "    grad = gradient(point[0], point[1])\n",
        "    new_point = point - learning_rate * grad  # Update rule\n",
        "\n",
        "    if np.linalg.norm(new_point - point) < tolerance:  # Convergence check\n",
        "        break\n",
        "    point = new_point\n",
        "\n",
        "print(\"Global minimum found at (x, y) =\", point)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3T107Qygqwf_",
        "outputId": "eb78c910-e135-4c85-f117-158004c3fbd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global minimum found at (x, y) = [4.01734511e-06 2.00867256e-06]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Rosenbrock function: f(x, y) = (1 - x)^2 + 100(y - x^2)^2\n",
        "def f(x, y):\n",
        "    return (1 - x)**2 + 100 * (y - x**2)**2\n",
        "\n",
        "# Gradient of the Rosenbrock function\n",
        "def gradient(x, y):\n",
        "    df_dx = -2*(1 - x) - 400*x*(y - x**2)\n",
        "    df_dy = 200*(y - x**2)\n",
        "    return np.array([df_dx, df_dy])\n",
        "\n",
        "# Gradient Descent parameters\n",
        "learning_rate = 0.001\n",
        "point = np.array([0, 0])  # Initial starting point (x0, y0)\n",
        "tolerance = 1e-6\n",
        "max_iterations = 10000\n",
        "\n",
        "# Gradient Descent loop\n",
        "for i in range(max_iterations):\n",
        "    grad = gradient(point[0], point[1])\n",
        "    new_point = point - learning_rate * grad  # Update rule\n",
        "\n",
        "    if np.linalg.norm(new_point - point) < tolerance:  # Convergence check\n",
        "        break\n",
        "    point = new_point\n",
        "\n",
        "print(\"Local minimum found at (x, y) =\", point)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcq3Ee0HqzIO",
        "outputId": "e085829e-e686-4b51-ede9-2d2a1115644d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local minimum found at (x, y) = [0.99440095 0.98881076]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# ReLU activation function\n",
        "def relu(Z):\n",
        "    return np.maximum(0, Z)\n",
        "\n",
        "# Derivative of ReLU for backpropagation\n",
        "def relu_derivative(Z):\n",
        "    return Z > 0\n",
        "\n",
        "# Softmax activation function for the output layer (classification)\n",
        "def softmax(Z):\n",
        "    exp_Z = np.exp(Z - np.max(Z))  # For numerical stability\n",
        "    return exp_Z / exp_Z.sum(axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "VSeXl6K4uZoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward propagation step\n",
        "def forward_propagation(X, W1, b1, W2, b2):\n",
        "    Z1 = np.dot(X, W1) + b1  # Hidden layer linear step\n",
        "    A1 = relu(Z1)            # Apply ReLU activation\n",
        "    Z2 = np.dot(A1, W2) + b2  # Output layer linear step\n",
        "    Y_hat = softmax(Z2)       # Apply softmax to get probabilities\n",
        "    return Z1, A1, Z2, Y_hat"
      ],
      "metadata": {
        "id": "r24DCLGjuZql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Backward propagation step\n",
        "def backward_propagation(X, Y, Z1, A1, Z2, Y_hat, W2):\n",
        "    m = X.shape[0]  # Number of samples\n",
        "\n",
        "    # Gradient of the loss with respect to Z2 (output layer)\n",
        "    dZ2 = Y_hat - Y\n",
        "\n",
        "    # Gradient with respect to W2 and b2\n",
        "    dW2 = np.dot(A1.T, dZ2) / m\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "    # Gradient of the activation with respect to Z1 (hidden layer)\n",
        "    dA1 = np.dot(dZ2, W2.T)\n",
        "    dZ1 = dA1 * relu_derivative(Z1)  # ReLU derivative for backprop\n",
        "\n",
        "    # Gradient with respect to W1 and b1\n",
        "    dW1 = np.dot(X.T, dZ1) / m\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "    return dW1, db1, dW2, db2"
      ],
      "metadata": {
        "id": "9QI_4VnquZtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train(X, Y, W1, b1, W2, b2, learning_rate, num_epochs):\n",
        "    for i in range(num_epochs):\n",
        "        # Forward propagation\n",
        "        Z1, A1, Z2, Y_hat = forward_propagation(X, W1, b1, W2, b2)\n",
        "\n",
        "        # Compute the loss (cross-entropy)\n",
        "        loss = -np.sum(Y * np.log(Y_hat + 1e-8)) / X.shape[0]\n",
        "        if i % 100 == 0:\n",
        "            print(f'Epoch {i}, Loss: {loss}')\n",
        "\n",
        "        # Backward propagation\n",
        "        dW1, db1, dW2, db2 = backward_propagation(X, Y, Z1, A1, Z2, Y_hat, W2)\n",
        "\n",
        "        # Update weights and biases\n",
        "        W1 -= learning_rate * dW1\n",
        "        b1 -= learning_rate * db1\n",
        "        W2 -= learning_rate * dW2\n",
        "        b2 -= learning_rate * db2\n",
        "\n",
        "    return W1, b1, W2, b2"
      ],
      "metadata": {
        "id": "yU9rQP1D7qBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XOR problem dataset\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input (4 samples, 2 features)\n",
        "Y = np.array([[1, 0], [0, 1], [0, 1], [1, 0]])  # Output (4 samples, 2 classes, one-hot encoded)\n"
      ],
      "metadata": {
        "id": "f96sJI5q7wMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights and biases randomly\n",
        "np.random.seed(42)  # Seed for reproducibility\n",
        "\n",
        "# 2 input features, 3 neurons in hidden layer, 2 output classes\n",
        "W1 = np.random.randn(2, 3) * 0.01\n",
        "b1 = np.zeros((1, 3))\n",
        "W2 = np.random.randn(3, 2) * 0.01\n",
        "b2 = np.zeros((1, 2))\n",
        "\n",
        "learning_rate = 0.01\n",
        "num_epochs = 10000"
      ],
      "metadata": {
        "id": "8_PaG2sb8QlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "W1, b1, W2, b2 = train(X, Y, W1, b1, W2, b2, learning_rate, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zosg3kP18Qob",
        "outputId": "b3bc2b1a-c562-4d11-afbd-0b617a082c65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.6931471686966064\n",
            "Epoch 100, Loss: 0.6931459426633895\n",
            "Epoch 200, Loss: 0.6931445641746916\n",
            "Epoch 300, Loss: 0.6931428589320954\n",
            "Epoch 400, Loss: 0.6931406117127277\n",
            "Epoch 500, Loss: 0.6931381335181085\n",
            "Epoch 600, Loss: 0.6931353477214024\n",
            "Epoch 700, Loss: 0.6931316969565677\n",
            "Epoch 800, Loss: 0.6931268983807521\n",
            "Epoch 900, Loss: 0.6931205747652627\n",
            "Epoch 1000, Loss: 0.693112552382283\n",
            "Epoch 1100, Loss: 0.6931025982437519\n",
            "Epoch 1200, Loss: 0.6930898511937645\n",
            "Epoch 1300, Loss: 0.6930735125467349\n",
            "Epoch 1400, Loss: 0.6930553480006876\n",
            "Epoch 1500, Loss: 0.6930329432957081\n",
            "Epoch 1600, Loss: 0.69300553801811\n",
            "Epoch 1700, Loss: 0.6929712522908049\n",
            "Epoch 1800, Loss: 0.6929288512136436\n",
            "Epoch 1900, Loss: 0.6928755963554181\n",
            "Epoch 2000, Loss: 0.6928099640021175\n",
            "Epoch 2100, Loss: 0.6927280922801112\n",
            "Epoch 2200, Loss: 0.6926267088683247\n",
            "Epoch 2300, Loss: 0.6924989128430565\n",
            "Epoch 2400, Loss: 0.6923410733283064\n",
            "Epoch 2500, Loss: 0.6921443259407292\n",
            "Epoch 2600, Loss: 0.691900370473917\n",
            "Epoch 2700, Loss: 0.691593326350419\n",
            "Epoch 2800, Loss: 0.6912135925818587\n",
            "Epoch 2900, Loss: 0.6907359027050539\n",
            "Epoch 3000, Loss: 0.690141738898819\n",
            "Epoch 3100, Loss: 0.6894083642586535\n",
            "Epoch 3200, Loss: 0.6884895071870184\n",
            "Epoch 3300, Loss: 0.6873426577058278\n",
            "Epoch 3400, Loss: 0.6859141734862435\n",
            "Epoch 3500, Loss: 0.6841572605396964\n",
            "Epoch 3600, Loss: 0.681955990715309\n",
            "Epoch 3700, Loss: 0.6792588714174992\n",
            "Epoch 3800, Loss: 0.6759192033490362\n",
            "Epoch 3900, Loss: 0.671836693874409\n",
            "Epoch 4000, Loss: 0.6668463124349784\n",
            "Epoch 4100, Loss: 0.6607697953403833\n",
            "Epoch 4200, Loss: 0.6535399011915964\n",
            "Epoch 4300, Loss: 0.6448660146589571\n",
            "Epoch 4400, Loss: 0.6348422377175039\n",
            "Epoch 4500, Loss: 0.6230621093206\n",
            "Epoch 4600, Loss: 0.6098454844870702\n",
            "Epoch 4700, Loss: 0.5952320102517501\n",
            "Epoch 4800, Loss: 0.579163223816823\n",
            "Epoch 4900, Loss: 0.5622984189995357\n",
            "Epoch 5000, Loss: 0.5449457754531981\n",
            "Epoch 5100, Loss: 0.5276904841038221\n",
            "Epoch 5200, Loss: 0.5108784738244267\n",
            "Epoch 5300, Loss: 0.4948362998339043\n",
            "Epoch 5400, Loss: 0.4796559751804226\n",
            "Epoch 5500, Loss: 0.46611169543416947\n",
            "Epoch 5600, Loss: 0.45376923896283106\n",
            "Epoch 5700, Loss: 0.4428177177227755\n",
            "Epoch 5800, Loss: 0.43306181746689687\n",
            "Epoch 5900, Loss: 0.42459463289825244\n",
            "Epoch 6000, Loss: 0.4169766876125707\n",
            "Epoch 6100, Loss: 0.4104328996340365\n",
            "Epoch 6200, Loss: 0.4045220160852925\n",
            "Epoch 6300, Loss: 0.3994726890913017\n",
            "Epoch 6400, Loss: 0.39504712371692297\n",
            "Epoch 6500, Loss: 0.39111548609054325\n",
            "Epoch 6600, Loss: 0.38766118753190815\n",
            "Epoch 6700, Loss: 0.38454666119485825\n",
            "Epoch 6800, Loss: 0.38180467595308776\n",
            "Epoch 6900, Loss: 0.3791902664825919\n",
            "Epoch 7000, Loss: 0.3770330971249746\n",
            "Epoch 7100, Loss: 0.3749895187378285\n",
            "Epoch 7200, Loss: 0.37325310461538486\n",
            "Epoch 7300, Loss: 0.37168177661810814\n",
            "Epoch 7400, Loss: 0.37004163905849546\n",
            "Epoch 7500, Loss: 0.36867791598062605\n",
            "Epoch 7600, Loss: 0.36748417287301127\n",
            "Epoch 7700, Loss: 0.36636992896932336\n",
            "Epoch 7800, Loss: 0.36541003909973757\n",
            "Epoch 7900, Loss: 0.36436294297307653\n",
            "Epoch 8000, Loss: 0.36356826559751415\n",
            "Epoch 8100, Loss: 0.3627677715912311\n",
            "Epoch 8200, Loss: 0.36199889764408594\n",
            "Epoch 8300, Loss: 0.36133941000258973\n",
            "Epoch 8400, Loss: 0.3606494632470579\n",
            "Epoch 8500, Loss: 0.3601291115926291\n",
            "Epoch 8600, Loss: 0.35955536974998525\n",
            "Epoch 8700, Loss: 0.3590374916926837\n",
            "Epoch 8800, Loss: 0.3585739431564794\n",
            "Epoch 8900, Loss: 0.35815477001624824\n",
            "Epoch 9000, Loss: 0.35769547196951856\n",
            "Epoch 9100, Loss: 0.357322825068532\n",
            "Epoch 9200, Loss: 0.35692197777643897\n",
            "Epoch 9300, Loss: 0.3565721040574978\n",
            "Epoch 9400, Loss: 0.356210394979752\n",
            "Epoch 9500, Loss: 0.3559090184296241\n",
            "Epoch 9600, Loss: 0.3556112471098194\n",
            "Epoch 9700, Loss: 0.3553704134413732\n",
            "Epoch 9800, Loss: 0.3551058936783219\n",
            "Epoch 9900, Loss: 0.35479982173658176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction function\n",
        "def predict(X, W1, b1, W2, b2):\n",
        "    _, _, _, Y_hat = forward_propagation(X, W1, b1, W2, b2)\n",
        "    predictions = np.argmax(Y_hat, axis=1)\n",
        "    return predictions\n",
        "\n",
        "# Test the model on the training data\n",
        "predictions = predict(X, W1, b1, W2, b2)\n",
        "print(\"Predictions:\", predictions)\n",
        "\n",
        "# Convert one-hot encoded Y to label format\n",
        "true_labels = np.argmax(Y, axis=1)\n",
        "print(\"True Labels:\", true_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzHcLN7F8QrO",
        "outputId": "12939559-d67b-41a7-be47-f85b197544ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [0 0 1 0]\n",
            "True Labels: [0 1 1 0]\n"
          ]
        }
      ]
    }
  ]
}