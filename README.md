## week1c1
#### 1. Use NumPy linear algebra package to find the solutions of the system of linear equations.
#### 2. Find the solution for the system of linear equations using elimination method.
#### 3. Evaluate the determinant of the matrix and examine the relationship between matrix singularity and number of solutions of the linear system.
## week2c1
#### 1. Perform row reduction to bring matrix into row echelon form.
#### 2. Find the solution for the system of linear equations using row reduced matrix.
#### 3. Evaluate the determinant of the matrix to see again the connection between matrix singularity and the number of solutions of the linear system.
## week3c1
#### 1. Translate a point in 2D space using a translation vector.
#### 2. Apply a scaling transformation to a 2D point using a scaling matrix.
#### 3. Perform a 2D rotation transformation using matrix multiplication.
#### 3.1. Example 1: Apply horizontal scaling to a point, scaling only the x-coordinate.
#### 3.2. Example 2: Reflect a 2D point across the y-axis using a reflection matrix.
#### 4. Apply scaling and rotation transformations to a square shape and visualize the results.
## week4c1
#### 1. Use Python to find eigenvalues and eigenvectors.
#### 2. Visualize and interpret eigenvalues and eigenvectos.
#### 3. Apply linear transformations, eigenvalues and eigenvectors in a webpage navigation model.
## week1c2
#### 1. Functions in Python my_function(x): Returns the result of the quadratic expression  x**2 + 2x + 1.
#### 2. Symbolic Differentiation.
#### 2.1. Introduction to Symbolic Computation with SymPy.
#### 2.2. sp.diff(expression, x): Computes the symbolic derivative of the expression with respect to x. and expression: Defines a symbolic quadratic expression  x**2 + 2x + 1 using SymPy.
#### 2.3. Limitations of Symbolic Differentiation.
#### 3. Numerical Differentiation.
#### 3.1. ( f( x0 + h ) - f( x0 )) / h: Approximates the numerical derivative of a function using the forward difference method.
#### 3.2. Limitations of Numerical Differentiation.
#### 4. Automatic Differentiation.
#### 4.1. Introduction to JAX.
#### 4.2. Automatic Differentiation with JAX.
#### 5. grad(f): Computes the derivative of function f(x) using JAXâ€™s automatic differentiation.
## week2c2
#### 1. Gradient Descent for One Variable with Global Minimum: Uses gradient descent to find the global minimum of f( x )=x ** 2.
#### 2. Gradient Descent for One Variable with Multiple Minima: Uses gradient descent to find a local minimum of f(x)=cos(x), depending on the initial starting point.
#### 3. Gradient Descent for Two Variables with Global Minimum: Applies gradient descent to minimize f( x , y )= x ** 2 + y ** 2 finding the global minmimum at ( 0 , 0 ).
#### 4. Gradient Descent for Two Variables with Multiple Minima: Uses gradient descent on the Rosenbrock function to find a local minimum in a complex landscape.
